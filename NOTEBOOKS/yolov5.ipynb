{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision import transforms\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "from modules.model_trainer import ModelTrainer\n",
    "from modules.model_performance import ModelPerformanceVisualizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GestureDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.classes = os.listdir(root_dir)\n",
    "        self.class_to_idx = {cls: idx for idx, cls in enumerate(self.classes)}\n",
    "        self.samples = self._load_samples()\n",
    "\n",
    "    def _load_samples(self):\n",
    "        samples = []\n",
    "        for cls in self.classes:\n",
    "            class_dir = os.path.join(self.root_dir, cls)\n",
    "            landmarks_dir = os.path.join(class_dir, 'LANDMARKS')\n",
    "            for landmark_file in os.listdir(landmarks_dir):\n",
    "                image_file = landmark_file.replace('_landmarks', '')\n",
    "                image_path = os.path.join(class_dir, image_file)\n",
    "                landmark_path = os.path.join(landmarks_dir, landmark_file)\n",
    "                samples.append((image_path, landmark_path, self.class_to_idx[cls]))\n",
    "        return samples\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path, landmark_path, label = self.samples[idx]\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        \n",
    "        with open(landmark_path, 'r') as f:\n",
    "            landmarks = json.load(f)\n",
    "        \n",
    "        landmarks = np.array(landmarks['landmarks']).flatten()\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, torch.tensor(landmarks, dtype=torch.float32), label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class YOLOv5GestureModel(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(YOLOv5GestureModel, self).__init__()\n",
    "        # Load YOLOv5 model\n",
    "        self.yolo = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True)\n",
    "        \n",
    "        # Get the feature extractor\n",
    "        self.feature_extractor = self.yolo.model.model[:9]  # Up to the 9th layer (adjust if needed)\n",
    "        \n",
    "        # Freeze YOLOv5 parameters\n",
    "        for param in self.feature_extractor.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        # Add custom layers for gesture recognition\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc1 = nn.Linear(512 + 42, 256)  # 512 from YOLOv5 feature extractor + 42 landmarks\n",
    "        self.fc2 = nn.Linear(256, num_classes)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, image, landmarks):\n",
    "        x = self.feature_extractor(image)\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = torch.cat((x, landmarks), dim=1)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\cloud/.cache\\torch\\hub\\ultralytics_yolov5_master\n",
      "YOLOv5  2024-8-20 Python-3.9.12 torch-2.3.1+cu121 CUDA:0 (NVIDIA GeForce GTX 1070, 8192MiB)\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients, 16.4 GFLOPs\n",
      "Adding AutoShape... \n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'DetectionModel' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 23\u001b[0m\n\u001b[0;32m     20\u001b[0m val_loader \u001b[38;5;241m=\u001b[39m DataLoader(val_dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m)\n\u001b[0;32m     22\u001b[0m num_classes \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m37\u001b[39m  \u001b[38;5;66;03m# Set to 37 classes\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mYOLOv5GestureModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Set up loss function and optimizer\u001b[39;00m\n\u001b[0;32m     26\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntrothpyLoss()\n",
      "Cell \u001b[1;32mIn[17], line 9\u001b[0m, in \u001b[0;36mYOLOv5GestureModel.__init__\u001b[1;34m(self, num_classes)\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39myolo \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mhub\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124multralytics/yolov5\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124myolov5s\u001b[39m\u001b[38;5;124m'\u001b[39m, pretrained\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Get the backbone feature extractor\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbackbone \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mSequential(\n\u001b[1;32m----> 9\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43myolo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m,  \u001b[38;5;66;03m# Conv\u001b[39;00m\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39myolo\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mmodel[\u001b[38;5;241m2\u001b[39m],  \u001b[38;5;66;03m# Conv\u001b[39;00m\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39myolo\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mmodel[\u001b[38;5;241m3\u001b[39m],  \u001b[38;5;66;03m# C3\u001b[39;00m\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39myolo\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mmodel[\u001b[38;5;241m4\u001b[39m],  \u001b[38;5;66;03m# Conv\u001b[39;00m\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39myolo\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mmodel[\u001b[38;5;241m5\u001b[39m],  \u001b[38;5;66;03m# C3\u001b[39;00m\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39myolo\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mmodel[\u001b[38;5;241m6\u001b[39m],  \u001b[38;5;66;03m# Conv\u001b[39;00m\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39myolo\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mmodel[\u001b[38;5;241m7\u001b[39m],  \u001b[38;5;66;03m# C3\u001b[39;00m\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39myolo\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mmodel[\u001b[38;5;241m8\u001b[39m],  \u001b[38;5;66;03m# Conv\u001b[39;00m\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39myolo\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mmodel[\u001b[38;5;241m9\u001b[39m],  \u001b[38;5;66;03m# SPP\u001b[39;00m\n\u001b[0;32m     18\u001b[0m )\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# Freeze YOLOv5 backbone parameters\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m param \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbackbone\u001b[38;5;241m.\u001b[39mparameters():\n",
      "\u001b[1;31mTypeError\u001b[0m: 'DetectionModel' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "trainer = ModelTrainer()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Set up data transforms\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((640, 640)),  # YOLOv5 default input size\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Create dataset\n",
    "full_dataset = GestureDataset(root_dir='DATASET', transform=transform)\n",
    "\n",
    "train_size = int(0.8 * len(full_dataset))\n",
    "val_size = len(full_dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False, num_workers=4)\n",
    "\n",
    "num_classes = 37  # Set to 37 classes\n",
    "model = YOLOv5GestureModel(num_classes).to(device)\n",
    "\n",
    "# Set up loss function and optimizer\n",
    "criterion = nn.CrossEntrothpyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "NUM_EPOCHS = 20\n",
    "EARLY_STOP = 10\n",
    "\n",
    "\n",
    "# Train the model\n",
    "TRAINED_MODEL, RESULTS = trainer.train(model=model, \n",
    "                                       train_loader=train_loader, \n",
    "                                       test_loader=val_loader, \n",
    "                                       optimizer=optimizer,\n",
    "                                       loss_fn=criterion,\n",
    "                                       epochs=NUM_EPOCHS,\n",
    "                                       scheduler=None,\n",
    "                                       patience=EARLY_STOP)\n",
    "\n",
    "visualizer = ModelPerformanceVisualizer(RESULTS)\n",
    "y_true, y_pred = visualizer.get_preds(model=TRAINED_MODEL, dataloader=val_loader, device=device)\n",
    "visualizer.plot_all(y_true=y_true, y_pred=y_pred, classes=num_classes, save_path=\"model_performance/plot.jpg\")\n",
    "\n",
    "# Save the model\n",
    "# torch.save(model.state_dict(), 'yolov5_gesture_recognition_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
